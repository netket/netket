{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using JAX as a backend in NetKet - Feature Preview for v3.0 \n",
    "\n",
    "In this tutorial we will show how differentiable functions (for example deep networks) written in [JAX](https://github.com/google/jax) can be used as variational quantum states in NetKet. \n",
    "\n",
    "This feature will be available in the upcoming major release (version 3.0). While version 3.0 is still in beta development, users can already try this feature. \n",
    "\n",
    "\n",
    "## Prerequisites \n",
    "\n",
    "To try out integration with JAX, you first need to fetch the beta version of NetKet v3 \n",
    "We recommend using a virtual environment (either a python environment or a conda environment), for example\n",
    "\n",
    "```shell\n",
    "python3 -m venv nk_env\n",
    "source nk_env/bin/activate\n",
    "pip install -U netket\n",
    "```\n",
    "\n",
    "## Defining the quantum system \n",
    "\n",
    "NetKet allows for full flexibility in defining quantum systems, for example when tackling a ground-state search problem. While there are a few pre-defined hamiltonians, it is relatively straightforward to implement new quantum operators/ Hamiltonians. \n",
    "\n",
    "In the following, we consider the case of a transverse-field Ising model defined on a graph with random edges. \n",
    "\n",
    "$$ H = -\\sum_{i\\in\\textrm{nodes}}^{L} \\sigma^x_{i} + J \\sum_{(i,j)\\in\\textrm{edges}}\\sigma_{i}^{z}\\sigma_{j}^{z} $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure we run on the CPU and not on the GPU\n",
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "\n",
    "import netket as nk\n",
    "\n",
    "#Define a random graph\n",
    "n_nodes=10\n",
    "n_edges=20\n",
    "from numpy.random import choice\n",
    "rand_edges=[choice(n_nodes, size=2,replace=False).tolist() for i in range(n_edges)]\n",
    "\n",
    "graph=nk.graph.Graph(nodes=[i for i in range(n_nodes)], edges=rand_edges)\n",
    "\n",
    "#Define the local hilbert space\n",
    "hi=nk.hilbert.Spin(s=0.5)**graph.n_nodes\n",
    "\n",
    "#Define the Hamiltonian as a sum of local operators \n",
    "from netket.operator import LocalOperator as Op\n",
    "\n",
    "# Pauli Matrices\n",
    "sx = [[0, 1], [1, 0]]\n",
    "sz = [[1, 0], [0, -1]]\n",
    "\n",
    "# Defining the Hamiltonian as a LocalOperator acting on the given Hilbert space\n",
    "ha = Op(hi)\n",
    "\n",
    "#Adding a transverse field term on each node of the graph\n",
    "for i in range(graph.n_nodes):\n",
    "    ha += Op(hi, sx, [i])\n",
    "\n",
    "#Adding nearest-neighbors interactions on the edges of the given graph\n",
    "from numpy import kron\n",
    "J=0.5\n",
    "for edge in graph.edges():\n",
    "    ha += J*Op(hi, kron(sz, sz), edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a JAX module to be used as a wave function\n",
    "\n",
    "We now want to define a suitable JAX wave function to be used as a wave function ansatz. To simplify the discusssion, we consider here a simple single-layer fully connected network with complex weights and a $tanh$ activation function. These are easy to define in JAX, using for example a model built with [STAX](https://github.com/google/jax/tree/master/jax/experimental). The only requirement is that these networks take as  inputs JAX arrays of shape ```(batch_size,n)```, where batch_size is an arbitrary ```batch size``` and ```n``` is the number of quantum degrees of freedom (for example, the number of spins, in the previous example). Notice that regardless of the dimensionality of the problem, the last dimension is always flattened into a single index.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax.example_libraries import stax\n",
    "# NOTE: for JAX version prior to 0.2.25, use the following import instead:\n",
    "# from jax.experimental import stax\n",
    "\n",
    "#We define a custom layer that performs the sum of its inputs \n",
    "def SumLayer():\n",
    "    def init_fun(rng, input_shape):\n",
    "        output_shape = (-1, 1)\n",
    "        return output_shape, ()\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        return inputs.sum(axis=-1)\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "#We construct a fully connected network with tanh activation \n",
    "model=stax.serial(stax.Dense(2 * graph.n_nodes, W_init=nk.nn.initializers.normal(stddev=0.1, dtype=complex), \n",
    "                             b_init=nk.nn.initializers.normal(stddev=0.1, dtype=complex)), \n",
    "                  stax.Tanh,SumLayer())\n",
    "\n",
    "# Alternatively, we could have used flax, which would have been easier:\n",
    "#class Model(nk.nn.Module):\n",
    "#    @nk.nn.compact\n",
    "#    def __call__(self, x):\n",
    "#        x = nk.nn.Dense(features=2*x.shape[-1], dtype=complex, kernel_init=nk.nn.initializers.normal(stddev=0.01), bias_init=nk.nn.initializers.normal(stddev=0.01))(x)\n",
    "#        x = jax.numpy.tanh(x)\n",
    "#        return jax.numpy.sum(x, axis=-1)   \n",
    "#model = Model()\n",
    "\n",
    "# Alternatively #2 we could have used the built in RBM model:\n",
    "#model = nk.models.RBM(alpha=2, use_visible_bias=False, dtype=np.complex128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network to find an approximate ground state\n",
    "\n",
    "In order to perform Variational Monte Carlo, we further need to specify a suitable \n",
    "sampler (to compute expectation values over the variational state) as well as \n",
    "an optimizer. In the following we will adopt the Stochatic Gradient Descent coupled\n",
    "with quantum natural gradients (this scheme is known in the VMC literature as Stochastic Reconfiguration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a sampler that performs local moves\n",
    "# NetKet automatically dispatches here to MCMC sampler written using JAX types\n",
    "sa = nk.sampler.MetropolisLocal(hilbert=hi, n_chains=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the variational state\n",
    "vs = nk.variational.MCState(sa, model, n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sgd\n",
    "# Also dispatching to JAX optimizer\n",
    "op = nk.optimizer.Sgd(learning_rate=0.01)\n",
    "\n",
    "# Using Stochastic Reconfiguration a.k.a. quantum natural gradient\n",
    "# Also dispatching to a pure JAX version\n",
    "sr = nk.optimizer.SR(diag_shift=0.01)\n",
    "\n",
    "# Create the Variational Monte Carlo instance to learn the ground state\n",
    "vmc = nk.VMC(\n",
    "    hamiltonian=ha, optimizer=op, variational_state=vs, preconditioner=sr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training loop \n",
    "\n",
    "The last version of NetKet also allows for a finer control of the vmc loop. In the simplest case, one can just iterate through the vmc object and print the current value of the energy. More sophisticated output schemes based on tensorboard have been also implemented, but are not discussed in this Tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.20-0.02j ± 0.12 [σ²=13.25, R̂=0.9994]\n",
      "50 -5.98-0.06j ± 0.14 [σ²=9.84, R̂=0.9992]\n",
      "100 -10.04-0.08j ± 0.11 [σ²=10.24, R̂=0.9995]\n",
      "150 -10.907+0.014j ± 0.042 [σ²=1.230, R̂=1.0001]\n",
      "200 -11.261-0.014j ± 0.034 [σ²=0.855, R̂=0.9993]\n",
      "250 -11.396-0.013j ± 0.024 [σ²=1.402, R̂=0.9993]\n",
      "300 -11.532+0.015j ± 0.015 [σ²=0.198, R̂=1.0000]\n",
      "350 -11.727-0.002j ± 0.019 [σ²=0.196, R̂=1.0028]\n",
      "400 -11.830-0.013j ± 0.017 [σ²=0.105, R̂=1.0051]\n",
      "450 -11.872+0.011j ± 0.010 [σ²=0.055, R̂=0.9991]\n"
     ]
    }
   ],
   "source": [
    "# Running the learning loop and printing the energy every 50 steps\n",
    "# [notice that the very first iteration is slow because of JIT compilation]\n",
    "for it in vmc.iter(500,50):\n",
    "    print(it,vmc.energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to exact diagonalization\n",
    "\n",
    "Since this is a relatively small quantum system, we can still diagonalize the Hamiltonian using exact diagonalization. For this purpose, NetKet conveniently exposes a ```.to_sparse``` method that just converts the Hamiltonian into a ```scipy``` sparse matrix.\n",
    "Here we first obtain this sparse matrix, and then diagonalize it with scipy builtins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact energy is :  -11.932889012463688\n",
      "Relative error is :  0.0034391959338140226\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "exact_ens=scipy.sparse.linalg.eigsh(ha.to_sparse(),k=1,which='SA',return_eigenvectors=False)\n",
    "print(\"Exact energy is : \",exact_ens[0])\n",
    "print(\"Relative error is : \", (abs((vmc.energy.mean-exact_ens[0])/exact_ens[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Netket development)",
   "language": "python",
   "name": "dev-netket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
