{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44701f85-3e69-4e1c-b017-54a843297568",
   "metadata": {},
   "source": [
    "# Defining Custom Operators\n",
    "\n",
    "In this page we will show how to define custom operators and the relevant methods used to compute expectation values and their gradients.\n",
    "This page assumes you have already read the [Hilbert module](Hilbert) and [Operator module](Operator) documentation and have a decent understanding of their class hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08155ce-49fc-41af-9762-75b95304b615",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install netket --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a7cca-cff2-425a-8b69-4ce44529bf7d",
   "metadata": {},
   "source": [
    "## Defining a custom _zero_ operator\n",
    "\n",
    "Let's assume you want to define an operator that always returns 0.\n",
    "That's a very useful operator!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d3ba54-c703-4ae9-ba6d-5c80791619e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netket as nk\n",
    "from netket.operator import AbstractOperator\n",
    "\n",
    "class ZeroOperator(AbstractOperator):\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed12709e-2705-4d3e-b4b9-29d583de6fdc",
   "metadata": {},
   "source": [
    "To define an operator we always need to define the `dtype` property, representing the dtype of the output. \n",
    "\n",
    "Moreover, since we did not define the `__init__()` method, we inherit the `AbstractOperator` init method which\n",
    "requires an hilbert space to be specified.\n",
    "First we define the hilbert space, then we construct the operator itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6984147f-20c6-4937-8c90-eed9d6fd1d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spin(s=1/2, N=4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi = nk.hilbert.Spin(0.5, 4); hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc64db1-99f5-46c7-925d-6e6cfcacec78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZeroOperator(hilbert=Spin(s=1/2, N=4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_op = ZeroOperator(hi); zero_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2c8dd-0a40-44c2-b085-8ccaff92c35c",
   "metadata": {},
   "source": [
    "If we define a variational state on the same hilbert space, and we try to compute the expectation value on that space, an error will be thrown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3051d8-5e9f-49f2-b4fa-6f9e76fb3b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "vs  = nk.vqs.MCState(nk.sampler.MetropolisLocal(hi), nk.models.RBM())\n",
    "\n",
    "# vs.expect(zero_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9340e2-e3d8-466b-815a-6f73ad9e6d98",
   "metadata": {},
   "source": [
    "The error should look roughly like the following:\n",
    "```python\n",
    ">>> vs.expect(zero_op)\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "  File \"/home/filippovicentini/Dropbox/Ricerca/Codes/Python/netket/netket/vqs/base.py\", line 164, in expect\n",
    "    return expect(self, Ô)\n",
    "  File \"plum/function.py\", line 537, in plum.function.Function.__call__\n",
    "  File \"/home/filippovicentini/Dropbox/Ricerca/Codes/Python/netket/netket/vqs/mc/mc_state/expect.py\", line 96, in expect\n",
    "    σ, args = get_local_kernel_arguments(vstate, Ô)\n",
    "  File \"plum/function.py\", line 536, in plum.function.Function.__call__\n",
    "  File \"plum/function.py\", line 501, in plum.function.Function.resolve_method\n",
    "  File \"plum/function.py\", line 435, in plum.function.Function.resolve_signature\n",
    "plum.function.NotFoundLookupError: For function \"get_local_kernel_arguments\", signature Signature(netket.vqs.mc.mc_state.state.MCState, __main__.ZeroOperator) could not be resolved.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee362e4-92b8-487c-8603-4bd5f38fadbb",
   "metadata": {},
   "source": [
    "This is because you defined a new operator, but you did not define how to compute an expectation value with it.\n",
    "The method to use to compute an expectation value is chosen from a list using multiple-dispatch, and it is defined both by the type of the variational state and by the type of the operator.\n",
    "\n",
    "To define the expect method you should do the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9144f214-6ed9-4aa4-b238-58b6c57500d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@nk.vqs.expect.dispatch\n",
    "def expect(vstate: nk.vqs.MCState, op: ZeroOperator):\n",
    "    return np.array(0, dtype=op.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542d1c9-43b5-4761-b19d-6126f5db3035",
   "metadata": {},
   "source": [
    "And if we now call again the expect method of the variational state, it will work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40498cb5-4ffa-46c9-bf7a-9fdb31abd23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs.expect(zero_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3c156-4037-4364-9d80-40f38cc1d47c",
   "metadata": {},
   "source": [
    "## Defining an operator from scratch\n",
    "\n",
    "Let's try to reimplement an operator from scratch. \n",
    "I will take the $\\hat{X} = \\sum_i^N \\hat{\\sigma}^{(X)}_i $ operator as a simple example.\n",
    "\n",
    "In Variational Monte Carlo, we usually compute expectation values through the following expectation value:\n",
    "\n",
    "$$ \\langle \\hat{X} \\rangle = \\langle \\psi | \\hat{X} | \\psi \\rangle = \\sum_\\sigma |\\psi(\\sigma)|^2 \\sum_{\\eta} \\frac{\\langle\\sigma|\\hat{X}|\\eta\\rangle\\langle\\eta|\\psi\\rangle}{\\langle \\sigma | \\psi\\rangle} = \\mathbb{E}_{\\sigma \\approx |\\psi(\\sigma)|^2}\\left[ E^{loc}(\\sigma)\\right]\n",
    "$$\n",
    "\n",
    "where $ E^{loc}(\\sigma) =  \\sum_{\\eta} \\frac{\\langle\\sigma|\\hat{X}|\\eta\\rangle\\langle\\eta|\\psi\\rangle}{\\langle \\sigma | \\psi\\rangle} $ is called the local estimator.\n",
    "\n",
    "First we define the operator itself:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "431d22d4-d650-430c-bd61-7556a95a9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOperator(AbstractOperator):\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    return float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd0ff6-cadd-46f6-8dfd-237cb92bb219",
   "metadata": {},
   "source": [
    "To then compute expectation values we need the following methods:\n",
    " \n",
    " - A method to sample the probability distribution $|\\psi(\\sigma)|^2$. This is already provided by monte carlo variational state interface and samples can be retrieved simply by calling {attr}`~netket.vqs.MCState.samples`.\n",
    " - A method to take the samples $ \\sigma $ and compute the connected elements $ \\eta $ so that $ \\langle\\sigma|\\hat{X}|\\eta\\rangle \\neq 0 $. This should also return those matrix elements.\n",
    " - A method to compute the local energy given the matrix elements, the $\\sigma$ and $ \\eta $ and the variational state.\n",
    " - The statistical average of the local energies.\n",
    "\n",
    "First we implement a method returning all the connected elements.\n",
    "Given a bitstring $ \\sigma $ for $N$ spins, the connected elements are $N$ bitstrings of the same length, where each one has a flipped bit.\n",
    "The matrix element is always 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fbe4e976-2b68-47cd-abc3-f6eabf100127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # partial(sum, axis=1)(x) == sum(x, axis=1)\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@jax.vmap\n",
    "def get_conns_and_mels(sigma):\n",
    "  # this code only works if sigma is a single bitstring\n",
    "  assert sigma.ndim == 1\n",
    "\n",
    "  # get number of spins\n",
    "  N = sigma.shape[-1]\n",
    "  # repeat eta N times\n",
    "  eta = jnp.tile(sigma, (N,1))\n",
    "  # diagonal indices\n",
    "  ids = np.diag_indices(N)\n",
    "  # flip those indices\n",
    "  eta = eta.at[ids].set(-eta.at[ids].get())\n",
    "  return eta, jnp.ones(N)\n",
    "\n",
    "@partial(jax.vmap, in_axes=(None, None, 0,0,0))\n",
    "def e_loc(logpsi, pars, sigma, eta, mels):\n",
    "  return jnp.sum(mels * jnp.exp(logpsi(pars, eta) - logpsi(pars, sigma)), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da8f8d-afe6-4e4f-b450-8060a2903d22",
   "metadata": {},
   "source": [
    "The first function takes a single bitstring $ \\sigma $, a vector with $N$ entries, and returns a batch of bitstrings $ eta_i $, a matrix $ N \\times N $ where every element in the diagonal is flipped.\n",
    "We then used `jax.vmap` to make this function work with batches of inputs $\\sigma$ (we could have written it from the beginning to work on batches, but this way the meaning is very clear).\n",
    "\n",
    "The other function also takes a single $ \\sigma $ and a batch of $ \\eta $ and their matrix elements, and uses the formula above to compute the local energy. \n",
    "This function is also `jax.vmap`ped in order to work with batches of inputs. The argument `in_axes=(None, None, 0,0,0)` means that the first 2 arguments do not change among batches, while the other three are batched along the first dimension.\n",
    "\n",
    "With those two functions written, we can write the expect method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f87909b-8438-4143-8673-11f5d7031d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nk.vqs.expect.dispatch\n",
    "def expect(vstate: nk.vqs.MCState, op: XOperator):\n",
    "    return _expect(vstate._apply_fun, vstate.variables, vstate.samples)\n",
    "\n",
    "@partial(jax.jit, static_argnums=0)\n",
    "def _expect(logpsi, variables, sigma):  \n",
    "    n_chains = sigma.shape[-2]\n",
    "    N = sigma.shape[-1]\n",
    "    # flatten all batches\n",
    "    sigma = sigma.reshape(-1, N)\n",
    "\n",
    "    eta, mels = get_conns_and_mels(sigma)\n",
    "\n",
    "    E_loc = e_loc(logpsi, variables, sigma, eta, mels)\n",
    "\n",
    "    # reshape back into chains to compute statistical information\n",
    "    E_loc = E_loc.reshape(-1, n_chains)\n",
    "\n",
    "    # this function computes things like variance and convergence information.\n",
    "    return nk.stats.statistics(E_loc.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d9d90-b893-4fc8-88c9-108738a39dfb",
   "metadata": {},
   "source": [
    "The dispatch rule is a thin layer that calls a jitted function, in order to have more speed.\n",
    "We cannot directly jit `expect` itself because it takes as input a `vstate`, which is not directly\n",
    "jit-compatible.\n",
    "\n",
    "\n",
    "The internal, jitted `_expect` takes as input the `vstate._apply_fun` function which is the one evaluating the neural quantum state, together with it's inputs (`vstate.variables`).\n",
    "Note that if you want to make the `expect_and_grad` method work with this custom operator, you will also have to define the dispatch rule for `expect_and_grad`.\n",
    "\n",
    "We can now test this operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "19305824-73bf-4778-81ff-7b5379f5985d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XOperator(hilbert=Spin(s=1/2, N=4))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.99940 ± 0.00074 [σ²=0.00055, R̂=1.0022]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_op = XOperator(hi)\n",
    "display(x_op)\n",
    "\n",
    "vs.expect(x_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e40fc-d1db-4ddd-ae1a-89c2dc67c3a4",
   "metadata": {},
   "source": [
    "We can compare it with the built-in LocalOperator implementation in NetKet whose indexing is written in Numba. Of course, as the operators are identical, the expectation values will match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c51ea005-2dd2-42c4-8922-d48eaf566e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalOperator(dim=4, acting_on=[(0,), (1,), (2,), (3,)], constant=0.0, dtype=float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.99940 ± 0.00074 [σ²=0.00055, R̂=1.0022]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_localop = sum([nk.operator.spin.sigmax(hi, i) for i in range(hi.size)])\n",
    "display(x_localop)\n",
    "\n",
    "vs.expect(x_op_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d8e32-3614-4923-93da-a12a47dd2b30",
   "metadata": {},
   "source": [
    "It might be interesting to investigate the performance difference among the two operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6cffc92e-0e0e-4415-bd10-4ac321bcedd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 µs ± 928 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.11 ms ± 4.35 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vs.expect(x_op)\n",
    "%timeit vs.expect(x_localop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2567f-bc77-4ecf-8cc9-b85477f74475",
   "metadata": {},
   "source": [
    "And you can see that our jax-based approach is more efficient than NetKet's built-in operators.\n",
    "This is for two reasons: first, NetKet's LocalOperator is a general object that can handle completely arbitrary operators, and this flexibility comes at the price of a slightly lower performance. \n",
    "Usually this is not an issue because if your model is complication enough, the cost of indexing the operator will be negligible with respect to the cost of evaluating the model itself.\n",
    "\n",
    "However, writing the operator in Jax has the added benefit that XLA (the Jax compiler) can introspect the code of the operator, and might uber-optimise the evaluation of the expectation value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264b0d6-c608-42cf-9486-d6ca6cde3e47",
   "metadata": {},
   "source": [
    "## Defining an operator the easy way\n",
    "\n",
    "Most operator that can be used efficiently with VMC approaches have the same structure as above, therefore, when defining new operators you will often find yourself redefining a similar function, where the only thing changing is the code to compute the connected elements, matrix elements, and the local kernel (in the case above computing E_loc).\n",
    "If you also want to define the gradient, the boilerplate becomes considerable.\n",
    "\n",
    "As such, to reduce the amount of boilerplate code that users must write when defining custom operators, NetKet will attempt by default to use a default expectation kernel.\n",
    "This kernel looks very similar to the `_expect` function above, and will call a function similar to `get_conns_and_mels` and a local kernel, selected using multiple dispatch.\n",
    "\n",
    "In the example below we show how to make use of this _lean_ interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "68c0e076-02e6-40df-8fc0-d5487a0c6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOperatorLean(AbstractOperator):\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return float\n",
    "    \n",
    "    @property\n",
    "    def is_hermitian(self):\n",
    "        return True\n",
    "\n",
    "def e_loc(logpsi, pars, sigma, extra_args):\n",
    "    eta, mels = extra_args\n",
    "    # check that sigma has been reshaped to 2D, eta is 3D\n",
    "    # sigma is (Nsamples, Nsites)\n",
    "    assert sigma.ndim == 2\n",
    "    # eta is (Nsamples, Nconnected, Nsites)\n",
    "    assert eta.ndim == 3\n",
    "    # let's write the local energy assuming a single sample, and vmap it\n",
    "    @partial(jax.vmap, in_axes=(0, 0, 0))\n",
    "    def _loc_vals(sigma, eta, mels):\n",
    "        return jnp.sum(mels * jnp.exp(logpsi(pars, eta) - logpsi(pars, sigma)), axis=-1)\n",
    "    \n",
    "    return _loc_vals(sigma, eta, mels)\n",
    "\n",
    "@nk.vqs.get_local_kernel.dispatch\n",
    "def get_local_kernel(vstate: nk.vqs.MCState, op: XOperatorLean):\n",
    "    return e_loc\n",
    "\n",
    "@nk.vqs.get_local_kernel_arguments.dispatch\n",
    "def get_local_kernel_arguments(vstate: nk.vqs.MCState, op: XOperatorLean):\n",
    "    sigma = vstate.samples\n",
    "    # get the connected elements. Reshape the samples because that code only works\n",
    "    # if the input is a 2D matrix\n",
    "    extra_args = get_conns_and_mels(sigma.reshape(-1, vstate.hilbert.size))\n",
    "    return sigma, extra_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b03835-c1d6-4708-b4f4-f92c8417936c",
   "metadata": {},
   "source": [
    "The first function here is very similar to the `e_loc` we defined in the previous section, however instead of taking `eta` and `mels` as two different arguments, they are passed as a tuple named `extra_args`, and must be unpacked by the kernel.\n",
    "That's because every operator has it's own `get_local_kernel_arguments` which prepares those extra_args, and every different operator might be passing different objects to the kernel.\n",
    "\n",
    "Also do note that the kernel is jit-compiled, therefore it must make use of `jax.numpy` functions, while the `get_local_kernel_argument` function *is not jitted*, and it is executed before jitting.\n",
    "This is in order to allow extra flexibility: sometimes your operators need some pre-processing code that is not jax-jittable, but is only numba-jittable, for example (this is the case of most operators in NetKet). \n",
    "\n",
    "All operators and super-operators in netket define those two methods. \n",
    "If you want to see some examples of how it is used internally, look at the source code found in [this folder](https://github.com/netket/netket/blob/master/netket/vqs/mc/MCState/expect.py).\n",
    "An additional benefit of using this latter definition, is that is automatically enables `expect_and_grad` for your custom operator.\n",
    "\n",
    "We can now test this new implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "58117d88-2c22-4a72-9f9a-b1db386bee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.99940 ± 0.00074 [σ²=0.00055, R̂=1.0022]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_op_lean = XOperatorLean(hi)\n",
    "vs.expect(x_op_lean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6be079-97e8-4d0a-b785-5dd4706ab894",
   "metadata": {},
   "source": [
    "## Comparison of the two approaches\n",
    "\n",
    "Above you've seen two different ways to define `expect`, but it might be unclear which one you should use in your code.\n",
    "In general, you should prefer the second one: the _simple_ interface is easier to use and enables by default gradients too.\n",
    "\n",
    "If you can express the expectation value of your operator in such a way that it can be estimated by simply averaging a single local-estimator the simple interface is best. \n",
    "We expect that you should be able to use this interface in the vast majority of cases. \n",
    "However in some cases you might want to try something particular, experiment, or your operator cannot be expressed in this form, or you want to try out some optimisations (such as chunking), or compute many operators at once.\n",
    "That's why we also allow you to override the general `expect` method alltogether: it allows a lot of extra flexibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Netket development)",
   "language": "python",
   "name": "dev-netket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
