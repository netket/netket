# Symmetric wave functions from equivariant neural networks

Exploiting symmetries in real-life data for improving and speeding up machine learning has long motivated using neural networks whose structure captures such symmetries. The best-known of such designs are convolutional neural networks (CNNs) that exploit the translation invariance of image data (i.e., the fact that a particular feature may appear anywhere on the image). The idea, however, is not to produce fully translation-invariant features early on. Instead, the output of each layer is *equivariant* with its input: if the latter is translated, the output will move along with it. This way, the output of progressive layers are all geometrically related to the original picture, and a final, translation-invariant, result can be generated by summing over all entries.

CNNs are naturally useful for representing eigenstates of regular lattice Hamiltonians, since they are exactly translation-invariant (or rather, have a well-defined wave vector). However, we can do even better by including the full space group of the Hamiltonian and constructing a convolutional network whose features are labelled by group elements rather than lattice sites. Such ans√§tze are called **group convolutional networks** (GCNNs): they were introduced in[^cohen] and first used in a neural-quantum-state context in[^roth].

[^cohen]: [Group Equivariant Convolutional Networks.](http://proceedings.mlr.press/v48/cohenc16.html) Taco Cohen, Max Welling. Proceedings of the 33rd International Conference on Machine Learning, PMLR 48:2990-2999, 2016.
[^roth]: [Group Convolutional Neural Networks Improve Quantum State Accuracy.](https://arxiv.org/abs/2104.05085) Christopher Roth, Allan H. MacDonald. arXiv:2104.05085.

GCNNs have two key building blocks:

* The first **group embedding layer** maps the input (that typically lives on a lattice) onto a set of features labelled by elements of the space group of the same lattice. These outputs can be thought of applying the same linear map to the input transformed with the several space-group symmetries.
* **Group equivariant layers** take such group-shaped features as inputs and map them on outputs of the same structure. They are equivariant with respect to the embedding above: composing an embedding and an equivariant layer should produce a valid embedding layer.

In addition, GCNNs may contain element-wise nonlinearities, since these do not interact with the group structure. [^cohen] also defines pooling and batching transformations compatible with group equivariance; these are less interesting in an NQS context.

## Constructing equivariant layers

Consider a conventional CNN on an infinite square lattice. The only linear maps that are equivariant with respect to an arbitrary translation of the input are convolutions of the form
$$ C_\vec{r} = \sum_{\vec r'} W_{\vec r'-\vec r} f_\vec{r} $$
[^cohen] motivates introducing GCNNs by reinterpreting the $\mathbb{Z}^2$ square lattice as the $\mathbb{Z}^2$ translation group of the same. In this language, we get

$$ C_g = \sum_h W_{g^{-1}h} f_h = \sum_h W_{hg^{-1}} f_h; $$ (eq:both_equiv)

the two forms are equivalent here because the group $\mathbb{Z}^2$ is Abelian. A general space group is, however, not, leading to equivariant layers that behave quite differently and play nice with different embedding layers.

### Covariant GCNNs, left-equivariant layers [introduced in [^cohen], implemented in NetKet]

In an NQS context, it is natural to require the outputs of the GCNN to behave like wave functions transformed according to the several space-group symmetries:
$$ |\psi_g\rangle = g|\psi\rangle $$
for some state $|\psi\rangle$. In terms of wave function components, this is equivalent to
$$ \psi_g(\boldsymbol\sigma) = \langle \boldsymbol\sigma | g|\psi\rangle = \psi(g^{-1}\boldsymbol\sigma). $$
We want to achieve this symmetry by constructing an embedding layer that maps $\boldsymbol\sigma$ onto a set of such features, and equivariant layers that preserve them. The first is a linear map of the input quantum numbers $\sigma_\vec{r}$:

$$ \begin{aligned}
    \mathbf{f}_g(\boldsymbol\sigma) &= \mathbf{f}(g^{-1}\boldsymbol\sigma) = \sum_\vec{r} \mathbf{W}_\vec{r} (g^{-1}\boldsymbol\sigma)_\vec{r} = \sum_\vec{r} \mathbf{W}_\vec{r} \sigma_{g\vec r} \\
    &= \sum_{\vec r} \mathbf{W}_{g^{-1} \vec r} \sigma_\vec{r},
\end{aligned} $$ (eq:cov_embed)

where $\mathbf{W}_{\vec r}$ is an arbitrary set of weights that live on the original lattice.
The third equality comes from the definition of space-group transforming a basis state: $(g\boldsymbol\sigma)_{g\vec r} \equiv \sigma_{\vec r}$.

It then turns out [^cohen] that this covariant embedding of basis states into the group structure is preserved only by the first form of {eq}`eq:both_equiv`:

$$ \boldsymbol\phi_g = \sum_{h} \mathbf{X}_{g^{-1}h} \mathbf{f}_h. $$ (eq:cov_equiv)

Indeed,
$$ \boldsymbol\phi_g = \sum_{h} \mathbf{X}_{g^{-1}h} \sum_{\vec r} \mathbf{W}_{h^{-1}\vec r} \sigma_\vec{r} = \sum_{\vec r}  \left( \sum_h \mathbf{X}_h \mathbf{W}_{h^{-1}(g^{-1}\vec r)}\right) \sigma_\vec{r}; $$
the expression in brackets only depends on $g^{-1}\vec r$, so it is equivalent to {eq}`eq:cov_embed`.
We also see that this layer is equivariant with respect to *multiplying from the left* (or *left-equivariant* for short):
if the inputs $\mathbf{f}_h$ are transformed into $\mathbf{f}'_h = \mathbf{f}_{uh}$, the output becomes
$$ \boldsymbol\phi'_g = \sum_h \mathbf{W}_{g^{-1}h} \mathbf{f}_{uh} = \sum_h \mathbf{W}_{g^{-1}u^{-1} h} \mathbf{f}_h = \boldsymbol\phi_{ug}. $$

As the conventional implementation of group equivariance in machine learning, NetKet implements the ingredients for a covariant GCNN:

* `DenseSymm` implements the embedding function {eq}`eq:cov_embed`;
* `DenseEquivariant` inplements the left-equivariant map {eq}`eq:cov_equiv`.

### Contravariant GCNNs, right-equivariant layers

An equally sensible alternative embedding of the input states in the group structure would be requiring that the coefficients of basis states transform according to the group transformation:
$$ \psi_g(\boldsymbol\sigma) = \psi(g\boldsymbol\sigma). $$
In analogy with covariant and contravariant vectors, we call the resulting GCNN *contravariant.* Repeating the steps above leads to the embedding layer

$$ \mathbf{f}_g(\boldsymbol\sigma) = \sum_{\vec r} \mathbf{W}_{g\vec r} \sigma_\vec{r}, $$ (eq:cont_embed)

which can be used with the equivariant layer

$$ \boldsymbol\phi_g = \sum_{h} \mathbf{X}_{hg^{-1}} \mathbf{f}_h. $$ (eq:cont_equiv)

As opposed to the covariant case, this layer is equivariant with respect to *multiplying on the right:*
$$ \boldsymbol\phi'_g = \sum_h \mathbf{W}_{hg^{-1}} \mathbf{f}_{hu} = \sum_h \mathbf{W}_{gu h^{-1}} \mathbf{f}_h = \boldsymbol\phi_{gu}. $$

## Odds and ends

* The equivariant layers are written more as a group correlation than a group convolution (i.e., something that involves a sum over group elements that multiply to $g$). This can easily be rectified by relabelling the kernels $\mathbf{X}_g$ to $\mathbf{X}_{g^{-1}}$ without changing the behaviour at all. In the covariant case, we get
$$ \boldsymbol\phi_g = \sum_{h} \mathbf{f}_h \mathbf{X}_{h^{-1}g}, $$
which still composes the right way with {eq}`eq:cov_embed`:
$$ \boldsymbol\phi_g = \sum_{h} \mathbf{X}_{h^{-1}g} \sum_{\vec r} \mathbf{W}_{h^{-1}\vec r} \sigma_\vec{r} = \sum_{\vec r}  \left( \sum_h \mathbf{X}_h \mathbf{W}_{h(g^{-1}\vec r)}\right) \sigma_\vec{r}. $$
However, given equivalence of the two layers, the convention set by [^cohen], and the existing implementation of {eq}`eq:cov_equiv` in NetKet, there is no advantage to switching to this convention.